# ğŸ” Google AI Studio - LLM Search Query Recommendation ğŸš€

<div align="center">
  <img src="https://media1.tenor.com/m/SWJNyLKohhkAAAAC/google-gif.gif" width="500" alt="Cool Google GIF">
</div>

**Purpose**: This project ğŸ’¡ was designed to cultivate a deep understanding of the data and technical skills ğŸ’» necessary to build a robust search engine ğŸŒ, similar to Googleâ€™s. Our primary objective was to develop an LLM capable of replicating the predictive autocomplete functionality âŒ¨ï¸ found in modern search engines, built entirely from scratch.

Explore our presentation slides [here](https://docs.google.com/presentation/d/1h4i1ClAScYdBZICnKjBKxB94AW4P4HrFIiRsZ9Y-uOQ/edit?usp=sharing)! ğŸ“š

## Meet the Team ğŸ§‘â€ğŸ¤â€ğŸ§‘

**Team Members**: Viswaretas Kotra, Rexford Nimoh

**Challenge Advisors & TAs**: Josh McAdams, Esther Lou

## Tech Stack ğŸ› ï¸

* Hugging Face ğŸ¤—
* Google Colab ğŸ§ª
* Pandas ğŸ¼
* PyTorch ğŸ”¥
* TikToken ğŸª™
* Visual Studio Code ğŸ’»

## Methodology & Implementation âš™ï¸

* **Dataset**: Sourced from Hugging Face: [nq\_open](https://huggingface.co/datasets/google-research-datasets/nq_open) ğŸ“Š
* **Model**: Transformer architecture with Multi-Head Attention, Feed Forward networks, and Linear layers. ğŸ§ 
* **Encoding**: Token and Positional Encoding âœï¸
* **Tokenization**: Two-layer encoding process using tiktoken with subsequent compression. å‹ç¼©
* **Data Split**: 90% training, 10% testing (holdout method) â—
* **Batching**: Implemented dataset batches to optimize training, minimize overfitting, and reduce memory usage. ğŸ“¦
* **Model Persistence**: Trained model parameters saved using PyTorch's `torch.save()` and are accessible [here](https://github.com/jsmnlao/Google-2B-Search-Query-Recommendation-System/blob/main/saved_bigram_language_model.pth). ğŸ’¾
* **Performance Measurement**: Log loss ğŸ“‰
* **Training Specs**: Due to resource constraints, training was conducted on a CPU for 25 hours, with 10,000 iterations over 4,000 training records. â±ï¸
* **Evaluation**: Final input/output evaluations are available in [Final\_Search\_Query\_Model.ipynb](https://github.com/jsmnlao/Google-2B-Search-Query-Recommendation-System/blob/main/Final_Search_Query_Model.ipynb) ğŸ“

## Quick Start: Running the Training Code â–¶ï¸

1.  **Clone the Repository**:

    ```bash
    git clone [https://github.com/jsmnlao/Google-2B-Search-Query-Recommendation-System.git](https://github.com/jsmnlao/Google-2B-Search-Query-Recommendation-System.git)
    ```

    OR

    ```bash
    git pull
    ```
2.  **Install Dependencies**:

    ```bash
    pip3 install -r requirements.txt
    ```
3.  **Execute Training**:

    ```bash
    python model.py
    ```

## Future Enhancements âœ¨

We aim to refine our model by incorporating personalized queries, capturing long-term dependencies (using LSTMs), and recognizing patterns in common search phrases. We also plan to include key metrics like precision and recall, train with larger datasets on GPUs, and conduct extensive hyperparameter tuning for optimal performance. ğŸš€
